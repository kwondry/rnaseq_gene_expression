import pandas as pd
import os
import logging
import math
from snakemake.utils import validate

configfile: "config/config.yaml"

path_to_sample_sheet = config["sample_sheet_path"]

validate(pd.read_csv(path_to_sample_sheet), "../config/sample_sheet_schema.yaml")

# split sample sheet with >1000 samples into batches
def split_sample_sheet(path):
    if os.path.isdir("./input/sample_sheet_batches/"):
        print("Sample sheet has already been separated into batches or processed as a single batch.")
    else:
        sample_sheet = pd.read_csv(path)
        batch_size_config = config["sample_sheet_batch_size"]# Default to 1000 if not in config

        os.makedirs("./input/sample_sheet_batches/", exist_ok=True)

        if sample_sheet.shape[0] > 2 * batch_size_config:
            print(f"Number of samples ({sample_sheet.shape[0]}) exceeds 2 * batch size ({batch_size_config}). Splitting into batches.")
            num_batches = math.ceil(sample_sheet.shape[0] / batch_size_config)
            # Sort by the 'sample' column
            sample_sheet = sample_sheet.sort_values(by='sample')
            # Create a row number (starting at 1 for consistency with R)
            sample_sheet['row_number'] = range(1, len(sample_sheet) + 1)
            # Calculate potentially_wrong_batch
            sample_sheet['potentially_wrong_batch'] = 1 + (sample_sheet['row_number'] / batch_size_config).apply(math.floor)
            # Group by sample and assign the 'batch' as the first potentially_wrong_batch for that sample
            sample_sheet['batch'] = sample_sheet.groupby('sample')['potentially_wrong_batch'].transform('first')
            # (Optional) If you don't need the intermediate columns anymore, you can drop them:
            sample_sheet = sample_sheet.drop(columns=['row_number', 'potentially_wrong_batch'])        
            # output CSV files for each batch
            for batch_number, group_df in sample_sheet.groupby('batch'):
                path_to_sample_sheet_batch = f"./input/sample_sheet_batches/sample_sheet_batch{batch_number}.csv"
                group_df.drop("batch", axis=1).to_csv(path_to_sample_sheet_batch, index=False)
        else:
            print(f"Number of samples ({sample_sheet.shape[0]}) is not greater than 2 * batch size ({batch_size_config}). Creating a single batch file.")
            path_to_sample_sheet_batch = f"./input/sample_sheet_batches/sample_sheet_batch1.csv"
            sample_sheet.to_csv(path_to_sample_sheet_batch, index=False)

split_sample_sheet(path_to_sample_sheet)

ALL_BATCHES = glob_wildcards("input/sample_sheet_batches/sample_sheet_{batch_num}.csv").batch_num

rule target:
    input:
        # expand("results/{batch_num}/multiqc/star_salmon/nfcore_rnaseq_{batch_num}_multiqc_report.html", batch_num=ALL_BATCHES)
        "results/combined_salmon.merged.gene_counts.tsv"

rule test:
    input:
        test_done = "test_results/multiqc/star_salmon/nfcore_rnaseq_test_multiqc_report.html"

def get_samples_for_batch(batch_num):
    raw_sample_sheet = pd.read_csv(f"input/sample_sheet_batches/sample_sheet_{batch_num}.csv")
    return raw_sample_sheet["sample"].tolist()

def get_fastqs_for_kraken(wildcards):
    raw_sample_sheet = pd.read_csv(path_to_sample_sheet).to_dict(orient="records")
    fastq_map = {rec["sample"]:{"forward_fastq":rec["fastq_1"], "reverse_fastq":rec["fastq_2"]} for rec in raw_sample_sheet}
    return {"forward_fastq":fastq_map[wildcards.sample]["forward_fastq"]}

rule download_kraken_db:
    output:
        db_file = f"{config['kraken_db_path']}/hash.k2d"
    params:
        db_path = config["kraken_db_path"],
        db_url = "https://genome-idx.s3.amazonaws.com/kraken/k2_standard_20250402.tar.gz"
    resources:
        cpus_per_task=1, 
        mem_mb=8000,
        runtime="8h",
        partition="short"
    shell:
        """
        mkdir -p {params.db_path}
        cd {params.db_path}
        wget {params.db_url}
        tar -xzf k2_standard_20250402.tar.gz
        rm k2_standard_20250402.tar.gz
        """

# Helper function to get genome info based on species
def get_genome_info(species):
    if species == "Homo sapiens":
        return "homo_sapiens", "GRCh38"
    elif species == "Mus musculus":
        return "mus_musculus", "GRCm39"
    else:
        raise ValueError(f"Unsupported species: {species}")

# Get genome info for configured species
species_dir, genome_id = get_genome_info(config.get("target_species", "Homo sapiens"))
ensembl_version = config.get("ensembl_version", 114)

rule download_reference:
    output:
        fasta = f"resources/reference/{species_dir}.{genome_id}.ensembl_{ensembl_version}.dna_sm.primary_assembly.fa.gz",
        gtf = f"resources/reference/{species_dir}.{genome_id}.ensembl_{ensembl_version}.gtf.gz"
    params:
        species = config.get("target_species", "Homo sapiens"),
        version = ensembl_version,
        species_dir = species_dir,
        genome_id = genome_id
    localrule: True
    shell:
        """
        mkdir -p resources/reference
        
        # Set variables for capitalized names in URLs
        SPECIES_DIR={params.species_dir}
        GENOME_ID={params.genome_id}
        
        # Download fasta
        wget -L -O {output.fasta} \
            ftp://ftp.ensembl.org/pub/release-{params.version}/fasta/{params.species_dir}/dna/${{SPECIES_DIR^}}.${{GENOME_ID}}.dna_sm.primary_assembly.fa.gz
        
        # Download GTF  
        wget -L -O {output.gtf} \
            ftp://ftp.ensembl.org/pub/release-{params.version}/gtf/{params.species_dir}/${{SPECIES_DIR^}}.${{GENOME_ID}}.{params.version}.gtf.gz
        """

# Only define kraken rule for human samples
if config.get("target_species", "Homo sapiens") == "Homo sapiens":
    rule kraken:
        input:
            unpack(get_fastqs_for_kraken),
            db_hash_file = f"{config['kraken_db_path']}/hash.k2d"
        output:
            report = "results/{batch_num}/kraken/{sample}.txt"
        conda:
            "envs/kraken.yaml"
        resources:
            cpus_per_task=8, 
            mem_mb=100000,
            runtime="1h",
            partition="short",
            serial_slots=1
        threads: 8
        params:
            kraken_db_path = config["kraken_db_path"]
        log:
            "logs/{batch_num}/kraken/{sample}.log"
        shell:
            """
            kraken2 --db {params.kraken_db_path} --gzip-compressed --threads {threads} \\
             {input.forward_fastq} --report {output.report} > /dev/null 2> {log}
            """

    def get_kraken_results(wildcards):
        return ["results/{{batch_num}}/kraken/{}.txt".format(s) for s in get_samples_for_batch(wildcards.batch_num)]

# Conditional rule selection based on target species
if config.get("target_species", "Homo sapiens") == "Homo sapiens":
    # For human samples: run Kraken filtering
    rule parse_kraken_make_nfcore_samplesheet:
        input:
            kraken_results = get_kraken_results,
            raw_sample_sheet = "input/sample_sheet_batches/sample_sheet_{batch_num}.csv"
        output:
            samplesheet = "results/{batch_num}/samples_filtered_for_nfcore.csv",
            kraken_stats = "results/{batch_num}/percent_target_by_sample.csv"
        conda:
            "envs/tidyverse.yaml"
        params:
            min_target_reads = config.get("min_target_reads", 100000),
            target_species = config.get("target_species", "Homo sapiens")
        resources:
            cpus_per_task=1,
            mem_mb=4000,
            runtime="8h",
            partition="short"
        script:
            "scripts/parse_kraken_make_nfcore_samplesheet.R"
else:
    # For mouse samples: skip Kraken, just copy the samplesheet
    rule copy_samplesheet_for_nfcore:
        input:
            raw_sample_sheet = "input/sample_sheet_batches/sample_sheet_{batch_num}.csv"
        output:
            samplesheet = "results/{batch_num}/samples_filtered_for_nfcore.csv",
            kraken_stats = "results/{batch_num}/percent_target_by_sample.csv"
        localrule: True
        shell:
            """
            cp {input.raw_sample_sheet} {output.samplesheet}
            # Create empty kraken_stats file for consistency
            echo "sample,pct,n_clade,n_taxon,tax_level,tax_id,tax_name,report_path" > {output.kraken_stats}
            """


rule rnaseq_pipeline:
    input:
        input = "results/{batch_num}/samples_filtered_for_nfcore.csv",
        fasta = f"resources/reference/{species_dir}.{genome_id}.ensembl_{ensembl_version}.dna_sm.primary_assembly.fa.gz",
        gtf = f"resources/reference/{species_dir}.{genome_id}.ensembl_{ensembl_version}.gtf.gz"
    output:
        gene_counts = "results/{batch_num}/star_salmon/salmon.merged.gene_counts.tsv",
        gene_tpm = "results/{batch_num}/star_salmon/salmon.merged.gene_tpm.tsv",
        counts_length_scaled = "results/{batch_num}/star_salmon/salmon.merged.gene_counts_length_scaled.tsv"
    retries: 4
    params:
        pipeline = "nf-core/rnaseq",
        revision = "3.15.0",
        profile = ["mamba"],
        aligner = "star_salmon",
        save_reference = False,
        fasta = lambda wildcards, input: input.fasta,
        gtf = lambda wildcards, input: input.gtf,
        min_trimmed_reads = 100000,
        save_align_intermeds = False,
        skip_deseq2_qc = True,
        skip_rseqc = True,
        skip_bigwig = True,
        skip_stringtie = True,
        skip_preseq = True,
        salmon_index = "/n/groups/kwon/data1/databases/rnaseq_index/index/salmon",
        multiqc_title = "nfcore_rnaseq_{batch_num}",
        extra = "-c workflow/rnaseq.config -w temp/work_{batch_num}",
        outdir = lambda wildcards, output: output.gene_counts.split("/star_salmon/")[0] + "/"
    resources:
        active_nextflow = 1,   # only runs two batches at a time
        cpus_per_task = 20,
        mem_mb = 200000,
        runtime = "72h",
        partition = "priority"
    handover: True
    wrapper:
        "v7.2.0/utils/nextflow"


localrules: cleanup_nextflow_work

rule cleanup_nextflow_work:
    input:
        gene_counts = "results/{batch_num}/star_salmon/salmon.merged.gene_counts.tsv"
    output:
        touch("results/{batch_num}/.cleanup_done")
    params:
        work_dir = "temp/work_{batch_num}"
    shell:
        """
        if [ -d "{params.work_dir}" ]; then
            rm -rf {params.work_dir}
            echo "Cleaned up {params.work_dir}"
        else
            echo "Directory {params.work_dir} does not exist, skipping cleanup"
        fi
        """

rule combining_batches:
    input:
        gene_counts = expand("results/{batch_num}/star_salmon/salmon.merged.gene_counts.tsv", batch_num = ALL_BATCHES),
        gene_tpm = expand("results/{batch_num}/star_salmon/salmon.merged.gene_tpm.tsv", batch_num = ALL_BATCHES),
        counts_length_scaled = expand("results/{batch_num}/star_salmon/salmon.merged.gene_counts_length_scaled.tsv", batch_num = ALL_BATCHES),
        cleanup_flags = expand("results/{batch_num}/.cleanup_done", batch_num = ALL_BATCHES)
    output:
        combined_gene_counts = "results/combined_salmon.merged.gene_counts.tsv",
        combined_gene_tpm = "results/combined_salmon.merged.gene_tpm.tsv",
        combined_counts_length_scaled = "results/combined_salmon.merged.gene_counts_length_scaled.tsv"
    conda:
        "envs/tidyverse.yaml"
    resources:
        cpus_per_task=2, 
        mem_mb=64000,
        runtime="8h",
        partition="short"
    script:
        "scripts/combining_batches.R"

rule rnaseq_pipeline_test:
    output:
        "test_results/multiqc/star_salmon/nfcore_rnaseq_test_multiqc_report.html",
    params:
        pipeline = "nf-core/rnaseq",
        revision = "3.20.0",
        profile = ["test", "mamba"],
        aligner = "star_salmon",
        save_reference = False,
        min_trimmed_reads = 1,
        save_align_intermeds = False,
        skip_deseq2_qc = True,
        skip_rseqc = True,
        skip_bigwig = True,
        skip_stringtie = True,
        skip_preseq = True,
        multiqc_title = "nfcore_rnaseq_test",
        extra = "-w temp/work -resume",
        outdir = lambda wildcards, output: str(Path(output[0]).parents[-2])
    handover: True
    wrapper:
        "v7.2.0/utils/nextflow"
